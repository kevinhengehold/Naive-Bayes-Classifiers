{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append('C:\\\\Users\\\\kevin\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\Scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"Sentiment\", \"Tweet_ID\", \"Date\", \"Query\", \"Handle\", \"Text\"]\n",
    "tweet_df = pd.read_csv(\"C:\\\\Users\\\\kevin\\\\Documents\\\\jupyter notebooks\\\\twitter\\\\sentiment training data\\\\training.1600000.processed.noemoticon.csv\", names=col_names, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Query</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment    Tweet_ID                          Date     Query  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "            Handle                                               Text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x24503d54040>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEICAYAAABiXeIWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dfZRV1Znn8e9PQMD4BoI0UmrpiAmiLZEKYswLhgjEtGL36Iiru8WWCZEYE+3MSsBkQkTp1jUZMWaiho4MaFqFEBMZu21FiMYkBCwICQISqoVINURKChGivBT9zB9nl1yKW8UtqFOXl99nrbPuOc/Ze599NkU9dV7uOYoIzMzM2tox5e6AmZkdmZxgzMwsF04wZmaWCycYMzPLhROMmZnlwgnGzMxy4QRj1oSkhyX9z3L3oy1JGifpTUnbJJ1S7v7Y0cEJxg4Lkj4m6VeStkiql/RLSR9pg3ZvlPSLwlhE3BwRdx1s2wfQl29J+mEO7XYC7gOGRcTxEbGpYN3HU9LZJulPkqJgeZukM9q6PwXbrkzb65jXNqy8/A9rhzxJJwLPAOOAWcCxwMeBHeXs12GkF9AFWN50RUS8DBwP2S98YA1wckQ0tGP/7AjlIxg7HJwLEBFPRMTuiHgvIp6PiN81FpB0k6SVkjZLek7SmQXrQtLNklan9d9Tph/wMHBJ+mv97VR+uqS70/wQSbWSvippo6QNkq6WdIWk36ejqTsKtnWMpPGS/l3SJkmzJHVP6xr/Yh8t6Q1Jb0n6elo3ArgDuC715bcpfqOk1yVtlbRG0l8XGyBJnSXdL2l9mu5PsXOBVanY25LmlzLgki6TtKxg+QVJiwqWfyHp6jR/mqQfS6pLffxSKeMB/LygX9skXSLpHEkvpSPVtyTNLKW/doiKCE+eDukJOBHYBMwAPgN0a7L+aqAG6Ed2VP4N4FcF64PsCOhk4AygDhiR1t0I/KJJe9OBu9P8EKAB+CbQCfhcqv84cALQH9gOnJ3K3wb8GqgAOgPfB55I6ypTX/4J6ApcSHYU1i+t/xbww4J+fAB4B/hgWu4N9G9mjCal7Z4K9AR+BdzVZLsd9zPO75cjO+J5D+iRlv8IrE/73DWtO4Xsj9TFaXyOBc4GXgeGt2I8Ohb04Qng66ndLsDHyv3z5+nAJx/B2CEvIt4BPsaeX851kuZI6pWKfB74x4hYGdmpnX8ABhQexQD3RMTbEfEG8DNgQCu6sAuYHBG7gCfJful+JyK2RsRyslNPf17Ql69HRG1E7CBLGtc0uc5wZ2RHYb8FfkuWaJrzn8D5krpGxIa0vWL+GpgUERsjog64E/jbVuzjXiJiO1ANfAKoAn4H/AK4FBgMrI7sWs5HgJ4RMSkidkbE62T/RqNSU6WMR6FdwJnAaRGxPSJ+0Uw5Oww4wdhhISWPGyOiAjgfOA24P60+E/iOpLfTaa56QECfgib+WDD/Lum6Q4k2RcTuNP9e+nyzYP17Be2dCfykoC8rgd1k10Fa1ZeI+BNwHXAzsEHSv0j6UDN9PA34Q8HyH1LsYLxEdgT3iTT/IvDJNL2UypwJnNa4v2mf72DP/pYyHoW+SvZvt0jSckk3HeQ+WBk5wdhhJyJeIzuNdX4KrQM+HxEnF0xdI+JXpTTXxt1bB3ymSV+6RMR/HEhfIuK5iLic7PTYa2RHB8WsJ/tl3uiMFDsYTRPMS+ybYNYBa5rs7wkRcUXB+ubGo9j+/jEiPhcRp5Ed/Two6ZyD3A8rEycYO+RJ+pCkr0iqSMunA9eTnduH7EL9BEn90/qTJF1bYvNvAhWSjm2j7j4MTG48PSepp6SRrehLpaRjUt1ekq6S9AGyazXbyP76L+YJ4Btpez3Irokc7C3PvwI+CAwCFqXTc2cCF7PnAv0i4B1JX5PUVVIHSedrzy3kLY1HHdkpwLMbNyjp2sZ/Z2AzWRJqbp/tEOcEY4eDrWS/1BZK+hNZYnkV+ApARPwEuBd4UtI7ad1nSmx7Ptk1lD9KeqsN+vodYA7wvKStqa8Xl1j3R+lzk6QlZP8/v0J2JFJPduTwhWbq3k12zeR3wDJgSYodsHSKbgmwPCJ2pvAC4A8RsTGV2Q1cSXZNaw3wFvAD4KRUvtnxiIh3gcnAL9MptMFk13QWStqW6n05ItYczH5Y+SjCLxwzM7O25yMYMzPLhROMmZnlwgnGzMxy4QRjZma58MMukx49ekRlZWW5u2FmdlhZvHjxWxHRs9g6J5iksrKS6urqcnfDzOywIukPza3zKTIzM8uFE4yZmeXCCcbMzHLhBGNmZrlwgjEzs1w4wZiZWS5yTTCSbk8vDXpV0hOSukjqLmmusvejz5XUraD8BEk1klZJGl4QHyhpWVr3gCSleGdJM1N8oaTKgjqj0zZWSxqd536amdm+ckswkvoAXwKqIuJ8oAPZa1THA/Mioi8wLy0j6by0vj8wguxFQx1Scw8BY4G+aRqR4mOAzRFxDjCF7JHtSOoOTCR7LPggYGJhIjMzs/zlfYqsI9A1vX/7OLL3WowEZqT1M4Cr0/xI4MmI2JHe/1ADDJLUGzgxIhZE9m6BR5vUaWxrNjA0Hd0MB+ZGRH1EbAbmsicpmZlZO8jtm/wR8R+Svg28QfbO8ucj4nlJvSJiQyqzQdKpqUof9ryhEKA2xXal+abxxjrrUlsNkrYApxTGi9R5n6SxZEdGnHHGGQext1A5/l8Oqv7haO09ny13F8zanP8vt508T5F1IzvCOAs4DfiApL9pqUqRWLQQP9A6ewIRUyOiKiKqevYs+igdMzM7QHmeIvs0sCYi6iJiF/AU8FHgzXTai/S5MZWvBU4vqF9BdkqtNs03je9VJ52GO4ns1bLNtWVmZu0kzwTzBjBY0nHpushQYCXZe7Yb7+oaDTyd5ucAo9KdYWeRXcxflE6nbZU0OLVzQ5M6jW1dA8xP12meA4ZJ6paOpIalmJmZtZM8r8EslDQbWAI0AL8BpgLHA7MkjSFLQtem8sslzQJWpPK3RMTu1Nw4YDrQFXg2TQCPAI9JqiE7chmV2qqXdBfwSio3KSLq89pXMzPbV66P64+IiWS3CxfaQXY0U6z8ZGBykXg1cH6R+HZSgiqybhowrZVdNjOzNuJv8puZWS6cYMzMLBdOMGZmlgsnGDMzy4UTjJmZ5cIJxszMcuEEY2ZmuXCCMTOzXDjBmJlZLpxgzMwsF04wZmaWCycYMzPLhROMmZnlwgnGzMxy4QRjZma5cIIxM7Nc5JZgJH1Q0tKC6R1Jt0nqLmmupNXps1tBnQmSaiStkjS8ID5Q0rK07oH06mTS65VnpvhCSZUFdUanbayWNBozM2tXuSWYiFgVEQMiYgAwEHgX+AkwHpgXEX2BeWkZSeeRvfK4PzACeFBSh9TcQ8BYoG+aRqT4GGBzRJwDTAHuTW11J3uT5sXAIGBiYSIzM7P8tdcpsqHAv0fEH4CRwIwUnwFcneZHAk9GxI6IWAPUAIMk9QZOjIgFERHAo03qNLY1Gxiajm6GA3Mjoj4iNgNz2ZOUzMysHbRXghkFPJHme0XEBoD0eWqK9wHWFdSpTbE+ab5pfK86EdEAbAFOaaEtMzNrJ7knGEnHAlcBP9pf0SKxaCF+oHUK+zZWUrWk6rq6uv10z8zMWqM9jmA+AyyJiDfT8pvptBfpc2OK1wKnF9SrANaneEWR+F51JHUETgLqW2hrLxExNSKqIqKqZ8+eB7yDZma2r/ZIMNez5/QYwByg8a6u0cDTBfFR6c6ws8gu5i9Kp9G2Shqcrq/c0KROY1vXAPPTdZrngGGSuqWL+8NSzMzM2knHPBuXdBxwOfD5gvA9wCxJY4A3gGsBImK5pFnACqABuCUidqc644DpQFfg2TQBPAI8JqmG7MhlVGqrXtJdwCup3KSIqM9lJ83MrKhcE0xEvEt20b0wtonsrrJi5ScDk4vEq4Hzi8S3kxJUkXXTgGmt77WZmbUFf5PfzMxy4QRjZma5cIIxM7NcOMGYmVkunGDMzCwXTjBmZpYLJxgzM8uFE4yZmeXCCcbMzHLhBGNmZrlwgjEzs1w4wZiZWS6cYMzMLBdOMGZmlgsnGDMzy4UTjJmZ5cIJxszMcpFrgpF0sqTZkl6TtFLSJZK6S5oraXX67FZQfoKkGkmrJA0viA+UtCyte0CSUryzpJkpvlBSZUGd0WkbqyWNznM/zcxsX3kfwXwH+LeI+BBwIbASGA/Mi4i+wLy0jKTzgFFAf2AE8KCkDqmdh4CxQN80jUjxMcDmiDgHmALcm9rqDkwELgYGARMLE5mZmeUvtwQj6UTgE8AjABGxMyLeBkYCM1KxGcDVaX4k8GRE7IiINUANMEhSb+DEiFgQEQE82qROY1uzgaHp6GY4MDci6iNiMzCXPUnJzMzaQZ5HMGcDdcD/lfQbST+Q9AGgV0RsAEifp6byfYB1BfVrU6xPmm8a36tORDQAW4BTWmhrL5LGSqqWVF1XV3cw+2pmZk3kmWA6AhcBD0XEh4E/kU6HNUNFYtFC/EDr7AlETI2Iqoio6tmzZwtdMzOz1sozwdQCtRGxMC3PJks4b6bTXqTPjQXlTy+oXwGsT/GKIvG96kjqCJwE1LfQlpmZtZPcEkxE/BFYJ+mDKTQUWAHMARrv6hoNPJ3m5wCj0p1hZ5FdzF+UTqNtlTQ4XV+5oUmdxrauAean6zTPAcMkdUsX94elmJmZtZOOObd/K/DPko4FXgf+jiypzZI0BngDuBYgIpZLmkWWhBqAWyJid2pnHDAd6Ao8mybIbiB4TFIN2ZHLqNRWvaS7gFdSuUkRUZ/njpqZ2d5yTTARsRSoKrJqaDPlJwOTi8SrgfOLxLeTElSRddOAaa3pr5mZtR1/k9/MzHLhBGNmZrlwgjEzs1w4wZiZWS6cYMzMLBdOMGZmlgsnGDMzy4UTjJmZ5cIJxszMcuEEY2ZmuXCCMTOzXDjBmJlZLpxgzMwsF04wZmaWCycYMzPLhROMmZnlwgnGzMxykWuCkbRW0jJJSyVVp1h3SXMlrU6f3QrKT5BUI2mVpOEF8YGpnRpJD0hSineWNDPFF0qqLKgzOm1jtaTRee6nmZntq6QEI2mf1xW3wmURMSAiGl+dPB6YFxF9gXlpGUnnAaOA/sAI4EFJHVKdh4CxQN80jUjxMcDmiDgHmALcm9rqDkwELgYGARMLE5mZmeWv1COYhyUtkvQFSScf5DZHAjPS/Azg6oL4kxGxIyLWADXAIEm9gRMjYkFEBPBokzqNbc0Ghqajm+HA3Iioj4jNwFz2JCUzM2sHJSWYiPgY8NfA6UC1pMclXV5KVeB5SYsljU2xXhGxIbW7ATg1xfsA6wrq1qZYnzTfNL5XnYhoALYAp7TQ1l4kjZVULam6rq6uhN0xM7NSdSy1YESslvQNoBp4APhwOlq4IyKeaqbapRGxXtKpwFxJr7WwCRXbbAvxA62zJxAxFZgKUFVVtc96MzM7cKVeg/lzSVOAlcCngCsjol+an9JcvYhYnz43Aj8hux7yZjrtRfrcmIrXkh0hNaoA1qd4RZH4XnUkdQROAupbaMvMzNpJqddg/g+wBLgwIm6JiCXwfgL5RrEKkj4g6YTGeWAY8CowB2i8q2s08HSanwOMSneGnUV2MX9ROo22VdLgdMR0Q5M6jW1dA8xP12meA4ZJ6pYu7g9LMTMzayelniK7AngvInYDSDoG6BIR70bEY83U6QX8JN1R3BF4PCL+TdIrwCxJY4A3gGsBImK5pFnACqABuKVxe8A4YDrQFXg2TQCPAI9JqiE7chmV2qqXdBfwSio3KSLqS9xXMzNrA6UmmBeATwPb0vJxwPPAR5urEBGvAxcWiW8ChjZTZzIwuUi8GtjnVumI2E5KUEXWTQOmNdc/MzPLV6mnyLpERGNyIc0fl0+XzMzsSFBqgvmTpIsaFyQNBN7Lp0tmZnYkKPUU2W3AjyQ13onVG7guny6ZmdmRoKQEExGvSPoQ8EGy75i8FhG7cu2ZmZkd1kr+oiXwEaAy1fmwJCLi0Vx6ZWZmh72SEoykx4D/AiwFGm8dbnwumJmZ2T5KPYKpAs5LX2I0MzPbr1LvInsV+LM8O2JmZkeWUo9gegArJC0CdjQGI+KqXHplZmaHvVITzLfy7ISZmR15Sr1N+SVJZwJ9I+IFSccBHfZXz8zMjl6lPq7/c2RvjPx+CvUBfppXp8zM7PBX6kX+W4BLgXcge/kYe95EaWZmto9SE8yOiNjZuJBe7uVbls3MrFmlJpiXJN0BdJV0OfAj4P/l1y0zMzvclZpgxgN1wDLg88C/0sybLM3MzKD0u8j+E/inNJmZme1XqXeRrZH0etOpxLodJP1G0jNpubukuZJWp89uBWUnSKqRtErS8IL4QEnL0roHlN7DLKmzpJkpvlBSZUGd0WkbqyWNLm04zMysrZR6iqyK7GnKHwE+DjwA/LDEul8GVhYsjwfmRURfYF5aRtJ5wCigPzACeFBS43dtHgLGAn3TNCLFxwCbI+IcYApwb2qrOzARuBgYBEwsTGRmZpa/khJMRGwqmP4jIu4HPrW/epIqgM8CPygIjwRmpPkZwNUF8ScjYkdErAFqgEGSegMnRsSC9LDNR5vUaWxrNjA0Hd0MB+ZGRH1EbAbmsicpmZlZOyj1cf0XFSweQ3ZEc0IJVe8HvtqkbK+I2AAQERskNX6fpg/w64JytSm2K803jTfWWZfaapC0BTilMF6kTuF+jSU7MuKMM84oYXfMzKxUpT6L7H8XzDcAa4H/1lIFSX8BbIyIxZKGlLANFYlFC/EDrbMnEDEVmApQVVXl7/WYmbWhUu8iu+wA2r4UuErSFUAX4ERJPwTelNQ7Hb30Bjam8rXA6QX1K4D1KV5RJF5YpzZ9+fMkoD7FhzSp8+IB7IOZmR2gUk+R/X1L6yPiviKxCcCEVH8I8D8i4m8k/S9gNHBP+nw6VZkDPC7pPuA0sov5iyJit6StkgYDC4EbgO8W1BkNLACuAeZHREh6DviHggv7wxr7YmZm7aM1b7T8CNkvdIArgZ+z93WOUt0DzJI0BngDuBYgIpZLmgWsIDsNd0tENL6eeRwwHegKPJsmgEeAxyTVkB25jEpt1Uu6C3gllZsUEfUH0FczMztArXnh2EURsRVA0reAH0XEfy+lckS8SDpFFRGbgKHNlJsMTC4SrwbOLxLfTkpQRdZNA6aV0j8zM2t7pX4P5gxgZ8HyTqCyzXtjZmZHjFKPYB4DFkn6CdndWH9J9n0UMzOzokq9i2yypGfJvsUP8HcR8Zv8umVmZoe7Uk+RARwHvBMR3yG7LfisnPpkZmZHgFIfdjkR+Bp7bvXtROnPIjMzs6NQqUcwfwlcBfwJICLWU9qjYszM7ChVaoLZmR40GQCSPpBfl8zM7EhQaoKZJen7wMmSPge8gF8+ZmZmLdjvXWTp8fczgQ8B7wAfBL4ZEXNz7puZmR3G9ptg0rO9fhoRA8neq2JmZrZfpZ4i+7Wkj+TaEzMzO6KU+k3+y4CbJa0lu5NMZAc3f55Xx8zM7PDWYoKRdEZEvAF8pp36Y2ZmR4j9HcH8lOwpyn+Q9OOI+K/t0SkzMzv87e8aTOGrh8/OsyNmZnZk2V+CiWbmzczMWrS/U2QXSnqH7Eima5qHPRf5T8y1d2Zmdthq8QgmIjpExIkRcUJEdEzzjcstJhdJXSQtkvRbScsl3Zni3SXNlbQ6fXYrqDNBUo2kVZKGF8QHSlqW1j2QvvyJpM6SZqb4QkmVBXVGp22sljT6wIbHzMwOVGse199aO4BPRcSFwABghKTBwHhgXkT0BealZSSdB4wC+gMjgAcldUhtPQSMBfqmaUSKjwE2R8Q5wBTg3tRWd2AicDEwCJhYmMjMzCx/uSWYyGxLi53SFMBIYEaKzwCuTvMjgScjYkdErAFqgEGSegMnRsSC9MDNR5vUaWxrNjA0Hd0MB+ZGRH1EbCZ7AkFjUjIzs3aQ5xEMkjpIWgpsJPuFvxDoFREbANLnqal4H2BdQfXaFOuT5pvG96oTEQ3AFuCUFtpq2r+xkqolVdfV1R3MrpqZWRO5JpiI2B0RA4AKsqOR81soriKxaCF+oHUK+zc1Iqoioqpnz54tdM3MzFor1wTTKCLeBl4kO031ZjrtRfrcmIrVAqcXVKsA1qd4RZH4XnUkdQROAupbaMvMzNpJbglGUk9JJ6f5rsCngdeAOUDjXV2jgafT/BxgVLoz7Cyyi/mL0mm0rZIGp+srNzSp09jWNcD8dJ3mOWCYpG7p4v6wFDMzs3ZS6sMuD0RvYEa6E+wYYFZEPCNpAdkLzMYAbwDXAkTEckmzgBVAA3BLROxObY0DpgNdgWfTBPAI8JikGrIjl1GprXpJdwGvpHKTIqI+x301M7MmckswEfE74MNF4puAoc3UmQxMLhKvBva5fhMR20kJqsi6acC01vXazMzaSrtcgzEzs6OPE4yZmeXCCcbMzHLhBGNmZrlwgjEzs1w4wZiZWS6cYMzMLBdOMGZmlgsnGDMzy4UTjJmZ5cIJxszMcuEEY2ZmuXCCMTOzXDjBmJlZLpxgzMwsF04wZmaWCycYMzPLRW4JRtLpkn4maaWk5ZK+nOLdJc2VtDp9diuoM0FSjaRVkoYXxAdKWpbWPSBJKd5Z0swUXyipsqDO6LSN1ZJG57WfZmZWXJ5HMA3AVyKiHzAYuEXSecB4YF5E9AXmpWXSulFAf2AE8KCkDqmth4CxQN80jUjxMcDmiDgHmALcm9rqDkwELgYGARMLE5mZmeUvtwQTERsiYkma3wqsBPoAI4EZqdgM4Oo0PxJ4MiJ2RMQaoAYYJKk3cGJELIiIAB5tUqexrdnA0HR0MxyYGxH1EbEZmMuepGRmZu2gXa7BpFNXHwYWAr0iYgNkSQg4NRXrA6wrqFabYn3SfNP4XnUiogHYApzSQltN+zVWUrWk6rq6ugPfQTMz20fuCUbS8cCPgdsi4p2WihaJRQvxA62zJxAxNSKqIqKqZ8+eLXTNzMxaK9cEI6kTWXL554h4KoXfTKe9SJ8bU7wWOL2gegWwPsUrisT3qiOpI3ASUN9CW2Zm1k7yvItMwCPAyoi4r2DVHKDxrq7RwNMF8VHpzrCzyC7mL0qn0bZKGpzavKFJnca2rgHmp+s0zwHDJHVLF/eHpZiZmbWTjjm2fSnwt8AySUtT7A7gHmCWpDHAG8C1ABGxXNIsYAXZHWi3RMTuVG8cMB3oCjybJsgS2GOSasiOXEaltuol3QW8kspNioj6vHbUzMz2lVuCiYhfUPxaCMDQZupMBiYXiVcD5xeJbyclqCLrpgHTSu2vmZm1LX+T38zMcuEEY2ZmuXCCMTOzXDjBmJlZLpxgzMwsF04wZmaWCycYMzPLhROMmZnlwgnGzMxy4QRjZma5cIIxM7NcOMGYmVkunGDMzCwXTjBmZpYLJxgzM8uFE4yZmeXCCcbMzHKRW4KRNE3SRkmvFsS6S5oraXX67FawboKkGkmrJA0viA+UtCyte0CSUryzpJkpvlBSZUGd0WkbqyWNzmsfzcyseXkewUwHRjSJjQfmRURfYF5aRtJ5wCigf6rzoKQOqc5DwFigb5oa2xwDbI6Ic4ApwL2pre7AROBiYBAwsTCRmZlZ+8gtwUTEz4H6JuGRwIw0PwO4uiD+ZETsiIg1QA0wSFJv4MSIWBARATzapE5jW7OBoenoZjgwNyLqI2IzMJd9E52ZmeWsva/B9IqIDQDp89QU7wOsKyhXm2J90nzT+F51IqIB2AKc0kJb+5A0VlK1pOq6urqD2C0zM2vqULnIryKxaCF+oHX2DkZMjYiqiKjq2bNnSR01M7PStHeCeTOd9iJ9bkzxWuD0gnIVwPoUrygS36uOpI7ASWSn5Jpry8zM2lHHdt7eHGA0cE/6fLog/rik+4DTyC7mL4qI3ZK2ShoMLARuAL7bpK0FwDXA/IgISc8B/1BwYX8YMCH/XbPDya5du6itrWX79u3l7soRp0uXLlRUVNCpU6dyd8XKLLcEI+kJYAjQQ1It2Z1d9wCzJI0B3gCuBYiI5ZJmASuABuCWiNidmhpHdkdaV+DZNAE8AjwmqYbsyGVUaqte0l3AK6ncpIhoerOBHeVqa2s54YQTqKysJN35bm0gIti0aRO1tbWcddZZ5e6OlVluCSYirm9m1dBmyk8GJheJVwPnF4lvJyWoIuumAdNK7qwddbZv3+7kkgNJnHLKKfimGYND5yK/WbtzcsmHx9UaOcGYmVku2vsiv9khqXL8v7Rpe2vv+WxJ5SZPnszjjz9Ohw4dOOaYY/j+97/PxRdf3KptLV26lPXr13PFFVcAMGfOHFasWMH48eNb3e9Svfjiixx77LF89KMfzW0bdvhzgjErkwULFvDMM8+wZMkSOnfuzFtvvcXOnTtb3c7SpUuprq5+P8FcddVVXHXVVW3d3b28+OKLHH/88U4w1iKfIjMrkw0bNtCjRw86d+4MQI8ePTjttNNYvHgxn/zkJxk4cCDDhw9nw4YNAAwZMoSvfe1rDBo0iHPPPZeXX36ZnTt38s1vfpOZM2cyYMAAZs6cyfTp0/niF78IwI033si4ceO47LLLOPvss3nppZe46aab6NevHzfeeOP7fXn++ee55JJLuOiii7j22mvZtm0bAJWVlUycOJGLLrqICy64gNdee421a9fy8MMPM2XKFAYMGMDLL7/cvgNnhw0nGLMyGTZsGOvWrePcc8/lC1/4Ai+99BK7du3i1ltvZfbs2SxevJibbrqJr3/96+/XaWhoYNGiRdx///3ceeedHHvssUyaNInrrruOpUuXct111+2znc2bNzN//nymTJnClVdeye23387y5ctZtmwZS5cu5a233uLuu+/mhRdeYMmSJVRVVXHfffe9X79Hjx4sWbKEcePG8e1vf5vKykpuvvlmbr/9dpYuXcrHP/7xdhkvO/z4FJlZmRx//PEsXryYl19+mZ/97Gdcd911fOMb3+DVV1/l8ssvB2D37t307t37/Tp/9Vd/BcDAgQNZu3ZtSdu58sorkcQFF1xAr169uOCCCwDo378/a9eupba2lhUrVnDppZcCsHPnTi655JKi23zqqacOer/t6OEEY1ZGHTp0YMiQIQwZMoQLLkBRg64AAAXFSURBVLiA733ve/Tv358FCxYULd94Oq1Dhw40NDSUtI3GOsccc8z7843LDQ0NdOjQgcsvv5wnnniizbZpBj5FZlY2q1atYvXq1e8vL126lH79+lFXV/d+gtm1axfLly9vsZ0TTjiBrVu3HnA/Bg8ezC9/+UtqamoAePfdd/n973+f6zbt6OAjGDNKv624LW3bto1bb72Vt99+m44dO3LOOecwdepUxo4dy5e+9CW2bNlCQ0MDt912G/3792+2ncsuu4x77rmHAQMGMGFC6x+717NnT6ZPn87111/Pjh07ALj77rs599xzm61z5ZVXcs011/D000/z3e9+19dhrChl7/GyqqqqqK6uPuD6bf09isNBOX4pt5WVK1fSr1+/cnfjiHU4j6//L7eOpMURUVVsnU+RmZlZLpxgzMwsF04wdtTy6eF8eFytkROMHZW6dOnCpk2b/MuwjTW+D6ZLly7l7oodAnwXmR2VKioqqK2t9XtLctD4RkszJxg7KnXq1MlvXDTL2RF9ikzSCEmrJNVIyu/Z5WZmto8jNsFI6gB8D/gMcB5wvaTzytsrM7OjxxGbYIBBQE1EvB4RO4EngZFl7pOZ2VHjSL4G0wdYV7BcC+z1qkBJY4GxaXGbpFUHsb0ewFsHUT8vufVL9x5U9aNuvA6S+9U67lcr6N6D6teZza04khOMisT2uic1IqYCU9tkY1J1c49LKCf3q3Xcr9Zxv1rnaOvXkXyKrBY4vWC5Alhfpr6YmR11juQE8wrQV9JZko4FRgFzytwnM7OjxhF7iiwiGiR9EXgO6ABMi4iWX6xxcNrkVFsO3K/Wcb9ax/1qnaOqX35cv5mZ5eJIPkVmZmZl5ARjZma5cIJphf09ekaZB9L630m66BDp1xBJWyQtTdM326lf0yRtlPRqM+vLNV7761e5xut0ST+TtFLScklfLlKm3cesxH61+5hJ6iJpkaTfpn7dWaRMOcarlH6V62esg6TfSHqmyLq2H6uI8FTCRHajwL8DZwPHAr8FzmtS5grgWbLv4AwGFh4i/RoCPFOGMfsEcBHwajPr2328SuxXucarN3BRmj8B+P0h8jNWSr/afczSGByf5jsBC4HBh8B4ldKvcv2M/T3weLFt5zFWPoIpXSmPnhkJPBqZXwMnS+p9CPSrLCLi50B9C0XKMV6l9KssImJDRCxJ81uBlWRPpCjU7mNWYr/aXRqDbWmxU5qa3rVUjvEqpV/tTlIF8FngB80UafOxcoIpXbFHzzT9T1ZKmXL0C+CSdMj+rKT+OfepVOUYr1KVdbwkVQIfJvvrt1BZx6yFfkEZxiyd8lkKbATmRsQhMV4l9Avaf7zuB74K/Gcz69t8rJxgSrffR8+UWKatlbLNJcCZEXEh8F3gpzn3qVTlGK9SlHW8JB0P/Bi4LSLeabq6SJV2GbP99KssYxYRuyNiANmTOgZJOr9JkbKMVwn9atfxkvQXwMaIWNxSsSKxgxorJ5jSlfLomXI8nma/24yIdxoP2SPiX4FOknrk3K9SHJKP8ynneEnqRPZL/J8j4qkiRcoyZvvrV7l/xiLibeBFYESTVWX9GWuuX2UYr0uBqyStJTuN/ilJP2xSps3HygmmdKU8emYOcEO6G2MwsCUiNpS7X5L+TJLS/CCyf/dNOferFOUYr/0q13ilbT4CrIyI+5op1u5jVkq/yjFmknpKOjnNdwU+DbzWpFg5xmu//Wrv8YqICRFRERGVZL8j5kfE3zQp1uZjdcQ+KqatRTOPnpF0c1r/MPCvZHdi1ADvAn93iPTrGmCcpAbgPWBUpNtG8iTpCbK7ZXpIqgUmkl3wLNt4ldivsowX2V+ZfwssS+fvAe4AzijoWznGrJR+lWPMegMzlL1c8BhgVkQ8U+7/kyX2q1w/Y3vJe6z8qBgzM8uFT5GZmVkunGDMzCwXTjBmZpYLJxgzM8uFE4yZmeXCCcbMzHLhBGNmZrn4/1oWiJuqiKOhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweet_df[['Sentiment']].plot.hist(bins=5, title=\"Sentiments of Tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    800000\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove mentions of other twitter handles\n",
    "preprocess_list = []\n",
    "tweet_preprocessed = ''\n",
    "\n",
    "for text in tweet_df['Text']:\n",
    "    #remove mentions of other twitter handles\n",
    "    tweet_preprocessed = re.sub(r'(^|[^@\\w])@(\\w{1,15})\\b', '', text)\n",
    "    #remove hashtags\n",
    "    tweet_preprocessed = re.sub(r'(^|[^#\\w])#\\w\\b', '', tweet_preprocessed)\n",
    "    #remove urls starting with http or https\n",
    "    tweet_preprocessed = re.sub(r'https{0,1}:\\/\\/\\S*', '', tweet_preprocessed)\n",
    "    #replace emojis with text\n",
    "    for emot in UNICODE_EMO:\n",
    "        tweet_preprocessed = tweet_preprocessed.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    \n",
    "    \n",
    "    preprocess_list.append(tweet_preprocessed)\n",
    "    \n",
    "tweet_df['Preprocessed Text'] = preprocess_list\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write this to a csv so we can start with it next time\n",
    "#tweet_df.to_csv(r'path\\to\\file\\preprocessed tweets 1_6M.csv', index=False)\n",
    "\n",
    "#use this line if you've already preprocessed the text and can pull it from an existing csv. \n",
    "#tweet_df = pd.read_csv(r'path\\to\\file\\preprocessed tweets 1_6M.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Look for preprocessed text that's nan.  if it exists, get rid of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999375"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df['Preprocessed Text'].count()/len(tweet_df['Preprocessed Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = tweet_df[tweet_df['Preprocessed Text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df['Preprocessed Text'].count()/len(tweet_df['Preprocessed Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Query</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Text</th>\n",
       "      <th>Preprocessed Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>- Awww, that's a bummer.  You shoulda got Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>I dived many times for the ball. Managed to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment    Tweet_ID                          Date     Query  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "            Handle                                               Text  \\\n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                   Preprocessed Text  \n",
       "0    - Awww, that's a bummer.  You shoulda got Da...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2   I dived many times for the ball. Managed to s...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4   no, it's not behaving at all. i'm mad. why am...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Naive Bayes with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sentence(sent):\n",
    "  return({word: True for word in nltk.word_tokenize(sent)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets_list = []\n",
    "neg_tweets_list = []\n",
    "\n",
    "for index, row in tweet_df.iterrows():\n",
    "    if row['Sentiment'] == 0:\n",
    "        neg_tweets_list.append([format_sentence(row['Preprocessed Text']), 'neg'])\n",
    "    elif row['Sentiment'] == 4:\n",
    "        pos_tweets_list.append([format_sentence(row['Preprocessed Text']), 'pos'])\n",
    "    else:\n",
    "        print(\"Row: \", str(index), \" is not positive or negative.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Not': True,\n",
       "  'sure': True,\n",
       "  'what': True,\n",
       "  'they': True,\n",
       "  'are': True,\n",
       "  ',': True,\n",
       "  'only': True,\n",
       "  'that': True,\n",
       "  'PoS': True,\n",
       "  '!': True,\n",
       "  'As': True,\n",
       "  'much': True,\n",
       "  'as': True,\n",
       "  'I': True,\n",
       "  'want': True,\n",
       "  'to': True,\n",
       "  'dont': True,\n",
       "  'think': True,\n",
       "  'can': True,\n",
       "  'trade': True,\n",
       "  'away': True,\n",
       "  'company': True,\n",
       "  'assets': True,\n",
       "  'sorry': True,\n",
       "  'andy': True},\n",
       " 'neg']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_tweets_list[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_docs(pos, neg, ratio):\n",
    "  train = pos[:int((1-ratio)*len(pos))] + neg[:int((1-ratio)*len(neg))]\n",
    "  test = pos[int((ratio)*len(pos)):] + neg[int((ratio)*len(neg)):]\n",
    "  return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train, Xy_test = split_docs(pos_tweets_list, neg_tweets_list, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                saddened = True              neg : pos    =     85.7 : 1.0\n",
      "                    Poem = True              pos : neg    =     80.3 : 1.0\n",
      "             squarespace = True              neg : pos    =     75.2 : 1.0\n",
      "                     228 = True              neg : pos    =     49.7 : 1.0\n",
      "                     447 = True              neg : pos    =     46.1 : 1.0\n",
      "                     ftl = True              neg : pos    =     37.7 : 1.0\n",
      "                    Died = True              neg : pos    =     37.0 : 1.0\n",
      "             shareholder = True              pos : neg    =     36.3 : 1.0\n",
      "           Disappointing = True              neg : pos    =     33.0 : 1.0\n",
      "                    sadd = True              neg : pos    =     32.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(Xy_train)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.util import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.795966852943727\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(classifier, Xy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Naive Bayes with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try this one without stop_words.\n",
    "#cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(tweet_df['Preprocessed Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, tweet_df['Sentiment'], test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB Accuracy: 78.26%\n",
      "CNB Accuracy: 78.26%\n",
      "BNB Accuracy: 77.78%\n"
     ]
    }
   ],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)\n",
    "\n",
    "predicted = MNB.predict(X_test)\n",
    "mnb_accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "\n",
    "CNB = ComplementNB()\n",
    "CNB.fit(X_train, Y_train)\n",
    "\n",
    "cnb_accuracy_score = metrics.accuracy_score(CNB.predict(X_test),Y_test)\n",
    "\n",
    "BNB = BernoulliNB()\n",
    "BNB.fit(X_train, Y_train)\n",
    "\n",
    "bnb_accuracy_score = metrics.accuracy_score(BNB.predict(X_test), Y_test)\n",
    "\n",
    "\n",
    "print(str('MNB Accuracy: {:04.2f}'.format(mnb_accuracy_score*100))+'%')\n",
    "print(str('CNB Accuracy: {:4.2f}'.format(cnb_accuracy_score*100))+'%')\n",
    "print(str('BNB Accuracy: {:4.2f}'.format(bnb_accuracy_score*100))+'%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with Stopwords:\n",
      "MNB Accuracy: 76.91%\n",
      "CNB Accuracy: 76.91%\n",
      "BNB Accuracy: 77.01%\n"
     ]
    }
   ],
   "source": [
    "#try with stopwords\n",
    "cv = CountVectorizer(stop_words='english', ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(tweet_df['Preprocessed Text'])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, tweet_df['Sentiment'], test_size=0.2, random_state=5)\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)\n",
    "predicted = MNB.predict(X_test)\n",
    "mnb_accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "\n",
    "CNB = ComplementNB()\n",
    "CNB.fit(X_train, Y_train)\n",
    "cnb_accuracy_score = metrics.accuracy_score(CNB.predict(X_test),Y_test)\n",
    "\n",
    "BNB = BernoulliNB()\n",
    "BNB.fit(X_train, Y_train)\n",
    "bnb_accuracy_score = metrics.accuracy_score(BNB.predict(X_test), Y_test)\n",
    "\n",
    "print('Naive Bayes with Stopwords:')\n",
    "print(str('MNB Accuracy: {:04.2f}'.format(mnb_accuracy_score*100))+'%')\n",
    "print(str('CNB Accuracy: {:4.2f}'.format(cnb_accuracy_score*100))+'%')\n",
    "print(str('BNB Accuracy: {:4.2f}'.format(bnb_accuracy_score*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with bigrams:\n",
      "MNB Accuracy: 78.95%\n",
      "CNB Accuracy: 78.95%\n",
      "BNB Accuracy: 79.14%\n"
     ]
    }
   ],
   "source": [
    "#try with bigrams\n",
    "cv = CountVectorizer(ngram_range = (2,2),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(tweet_df['Preprocessed Text'])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, tweet_df['Sentiment'], test_size=0.2, random_state=5)\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)\n",
    "predicted = MNB.predict(X_test)\n",
    "mnb_accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "\n",
    "CNB = ComplementNB()\n",
    "CNB.fit(X_train, Y_train)\n",
    "cnb_accuracy_score = metrics.accuracy_score(CNB.predict(X_test),Y_test)\n",
    "\n",
    "BNB = BernoulliNB()\n",
    "BNB.fit(X_train, Y_train)\n",
    "bnb_accuracy_score = metrics.accuracy_score(BNB.predict(X_test), Y_test)\n",
    "\n",
    "print('Naive Bayes with bigrams:')\n",
    "print(str('MNB Accuracy: {:04.2f}'.format(mnb_accuracy_score*100))+'%')\n",
    "print(str('CNB Accuracy: {:4.2f}'.format(cnb_accuracy_score*100))+'%')\n",
    "print(str('BNB Accuracy: {:4.2f}'.format(bnb_accuracy_score*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with 1-grams, bigrams:\n",
      "MNB Accuracy: 80.34%\n",
      "CNB Accuracy: 80.34%\n",
      "BNB Accuracy: 79.90%\n"
     ]
    }
   ],
   "source": [
    "#try with 1-grams, bigrams\n",
    "cv = CountVectorizer(ngram_range = (1,2),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(tweet_df['Preprocessed Text'])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, tweet_df['Sentiment'], test_size=0.2, random_state=5)\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)\n",
    "predicted = MNB.predict(X_test)\n",
    "mnb_accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "\n",
    "CNB = ComplementNB()\n",
    "CNB.fit(X_train, Y_train)\n",
    "cnb_accuracy_score = metrics.accuracy_score(CNB.predict(X_test),Y_test)\n",
    "\n",
    "BNB = BernoulliNB()\n",
    "BNB.fit(X_train, Y_train)\n",
    "bnb_accuracy_score = metrics.accuracy_score(BNB.predict(X_test), Y_test)\n",
    "\n",
    "print('Naive Bayes with 1-grams, bigrams:')\n",
    "print(str('MNB Accuracy: {:04.2f}'.format(mnb_accuracy_score*100))+'%')\n",
    "print(str('CNB Accuracy: {:4.2f}'.format(cnb_accuracy_score*100))+'%')\n",
    "print(str('BNB Accuracy: {:4.2f}'.format(bnb_accuracy_score*100))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Use TF-IDF to transform tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with Tf-idf, 1-grams:\n",
      "MNB Accuracy: 77.67%\n",
      "CNB Accuracy: 77.66%\n",
      "BNB Accuracy: 77.78%\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_count_2 = tfidf.fit_transform(tweet_df['Preprocessed Text'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_count_2, tweet_df['Sentiment'],test_size=0.2,random_state=5)\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(x_train, y_train)\n",
    "predicted = MNB.predict(x_test)\n",
    "mnb_accuracy_score = metrics.accuracy_score(predicted, y_test)\n",
    "\n",
    "CNB = ComplementNB()\n",
    "CNB.fit(x_train, y_train)\n",
    "cnb_accuracy_score = metrics.accuracy_score(CNB.predict(x_test),y_test)\n",
    "\n",
    "BNB = BernoulliNB()\n",
    "BNB.fit(x_train, y_train)\n",
    "bnb_accuracy_score = metrics.accuracy_score(BNB.predict(x_test), y_test)\n",
    "\n",
    "print('Naive Bayes with Tf-idf, 1-grams:')\n",
    "print(str('MNB Accuracy: {:04.2f}'.format(mnb_accuracy_score*100))+'%')\n",
    "print(str('CNB Accuracy: {:4.2f}'.format(cnb_accuracy_score*100))+'%')\n",
    "print(str('BNB Accuracy: {:4.2f}'.format(bnb_accuracy_score*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with Tf-idf, 1-grams, stop words:\n",
      "MNB Accuracy: 76.32%\n",
      "CNB Accuracy: 76.32%\n",
      "BNB Accuracy: 77.01%\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_count_2 = tfidf.fit_transform(tweet_df['Preprocessed Text'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_count_2, tweet_df['Sentiment'],test_size=0.2,random_state=5)\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(x_train, y_train)\n",
    "predicted = MNB.predict(x_test)\n",
    "mnb_accuracy_score = metrics.accuracy_score(predicted, y_test)\n",
    "\n",
    "CNB = ComplementNB()\n",
    "CNB.fit(x_train, y_train)\n",
    "cnb_accuracy_score = metrics.accuracy_score(CNB.predict(x_test),y_test)\n",
    "\n",
    "BNB = BernoulliNB()\n",
    "BNB.fit(x_train, y_train)\n",
    "bnb_accuracy_score = metrics.accuracy_score(BNB.predict(x_test), y_test)\n",
    "\n",
    "print('Naive Bayes with Tf-idf, 1-grams, stop words:')\n",
    "print(str('MNB Accuracy: {:04.2f}'.format(mnb_accuracy_score*100))+'%')\n",
    "print(str('CNB Accuracy: {:4.2f}'.format(cnb_accuracy_score*100))+'%')\n",
    "print(str('BNB Accuracy: {:4.2f}'.format(bnb_accuracy_score*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with Tf-idf, 1-grams and 2-grams:\n",
      "MNB Accuracy: 80.53%\n",
      "CNB Accuracy: 80.53%\n",
      "BNB Accuracy: 79.90%\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range = (1,2),tokenizer = token.tokenize)\n",
    "text_count_2 = tfidf.fit_transform(tweet_df['Preprocessed Text'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_count_2, tweet_df['Sentiment'],test_size=0.2,random_state=5)\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(x_train, y_train)\n",
    "predicted = MNB.predict(x_test)\n",
    "mnb_accuracy_score = metrics.accuracy_score(predicted, y_test)\n",
    "\n",
    "CNB = ComplementNB()\n",
    "CNB.fit(x_train, y_train)\n",
    "cnb_accuracy_score = metrics.accuracy_score(CNB.predict(x_test),y_test)\n",
    "\n",
    "BNB = BernoulliNB()\n",
    "BNB.fit(x_train, y_train)\n",
    "bnb_accuracy_score = metrics.accuracy_score(BNB.predict(x_test), y_test)\n",
    "\n",
    "print('Naive Bayes with Tf-idf, 1-grams and 2-grams:')\n",
    "print(str('MNB Accuracy: {:04.2f}'.format(mnb_accuracy_score*100))+'%')\n",
    "print(str('CNB Accuracy: {:4.2f}'.format(cnb_accuracy_score*100))+'%')\n",
    "print(str('BNB Accuracy: {:4.2f}'.format(bnb_accuracy_score*100))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Try stemming the tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "porter = PorterStemmer()\n",
    "updated_tweets = []\n",
    "\n",
    "for text in tweet_df['Preprocessed Text']:\n",
    "    tokenize_tweet = word_tokenize(text)\n",
    "    stemmed_text = []\n",
    "    for word in tokenize_tweet:\n",
    "        stemmed_text.append(porter.stem(word))\n",
    "        stemmed_text.append(\" \")\n",
    "    updated_tweets.append(\"\".join(stemmed_text))\n",
    "\n",
    "tweet_df['Stemmed Text'] = updated_tweets\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I wa live unsubstanti in an underground bunker smile , eat , and mastic \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "porter = PorterStemmer()\n",
    "\n",
    "test_string = \"I was living unsubstantially in an underground bunker smiling, eating, and masticating\"\n",
    "tokenize_test = word_tokenize(test_string)\n",
    "stem_test = []\n",
    "for word in tokenize_test:\n",
    "    stem_test.append(porter.stem(word))\n",
    "    stem_test.append(\" \")\n",
    "    \n",
    "print(\"\".join(stem_test))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Query</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Text</th>\n",
       "      <th>Preprocessed Text</th>\n",
       "      <th>Stemmed Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>- Awww, that's a bummer.  You shoulda got Da...</td>\n",
       "      <td>- awww , that 's a bummer . you shoulda got da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he ca n't updat hi facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>I dived many times for the ball. Managed to s...</td>\n",
       "      <td>I dive mani time for the ball . manag to save ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole bodi feel itchi and like it on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am...</td>\n",
       "      <td>no , it 's not behav at all . i 'm mad . whi a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment    Tweet_ID                          Date     Query  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "            Handle                                               Text  \\\n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                   Preprocessed Text  \\\n",
       "0    - Awww, that's a bummer.  You shoulda got Da...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2   I dived many times for the ball. Managed to s...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4   no, it's not behaving at all. i'm mad. why am...   \n",
       "\n",
       "                                        Stemmed Text  \n",
       "0  - awww , that 's a bummer . you shoulda got da...  \n",
       "1  is upset that he ca n't updat hi facebook by t...  \n",
       "2  I dive mani time for the ball . manag to save ...  \n",
       "3      my whole bodi feel itchi and like it on fire   \n",
       "4  no , it 's not behav at all . i 'm mad . whi a...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I hope they will increas the capac fast , yesterday wa such a pain . got the fail whale +15 time in 2 hour .... '"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df['Stemmed Text'][101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Naive Bayes:\n",
      "MNB Accuracy: 77.83%\n",
      "CNB Accuracy: 77.83%\n",
      "BNB Accuracy: 77.12%\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(tweet_df['Stemmed Text'])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, tweet_df['Sentiment'], test_size=0.2, random_state=5)\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)\n",
    "predicted = MNB.predict(X_test)\n",
    "mnb_accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "\n",
    "CNB = ComplementNB()\n",
    "CNB.fit(X_train, Y_train)\n",
    "cnb_accuracy_score = metrics.accuracy_score(CNB.predict(X_test),Y_test)\n",
    "\n",
    "BNB = BernoulliNB()\n",
    "BNB.fit(X_train, Y_train)\n",
    "bnb_accuracy_score = metrics.accuracy_score(BNB.predict(X_test), Y_test)\n",
    "\n",
    "print('Stemmed Naive Bayes:')\n",
    "print(str('MNB Accuracy: {:04.2f}'.format(mnb_accuracy_score*100))+'%')\n",
    "print(str('CNB Accuracy: {:4.2f}'.format(cnb_accuracy_score*100))+'%')\n",
    "print(str('BNB Accuracy: {:4.2f}'.format(bnb_accuracy_score*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Naive Bayes with Stopwords:\n",
      "MNB Accuracy: 76.56%\n",
      "CNB Accuracy: 76.56%\n",
      "BNB Accuracy: 76.63%\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words='english', ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(tweet_df['Stemmed Text'])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, tweet_df['Sentiment'], test_size=0.2, random_state=5)\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)\n",
    "predicted = MNB.predict(X_test)\n",
    "mnb_accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "\n",
    "CNB = ComplementNB()\n",
    "CNB.fit(X_train, Y_train)\n",
    "cnb_accuracy_score = metrics.accuracy_score(CNB.predict(X_test),Y_test)\n",
    "\n",
    "BNB = BernoulliNB()\n",
    "BNB.fit(X_train, Y_train)\n",
    "bnb_accuracy_score = metrics.accuracy_score(BNB.predict(X_test), Y_test)\n",
    "\n",
    "print('Stemmed Naive Bayes with Stopwords:')\n",
    "print(str('MNB Accuracy: {:04.2f}'.format(mnb_accuracy_score*100))+'%')\n",
    "print(str('CNB Accuracy: {:4.2f}'.format(cnb_accuracy_score*100))+'%')\n",
    "print(str('BNB Accuracy: {:4.2f}'.format(bnb_accuracy_score*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Naive Bayes with 1-gram and 2-gram:\n",
      "MNB Accuracy: 80.11%\n",
      "CNB Accuracy: 80.11%\n",
      "BNB Accuracy: 79.54%\n"
     ]
    }
   ],
   "source": [
    "# 1-gram, 2-gram stemmed tweets\n",
    "\n",
    "cv = CountVectorizer(ngram_range = (1,2),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(tweet_df['Stemmed Text'])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, tweet_df['Sentiment'], test_size=0.2, random_state=5)\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)\n",
    "predicted = MNB.predict(X_test)\n",
    "mnb_accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "\n",
    "CNB = ComplementNB()\n",
    "CNB.fit(X_train, Y_train)\n",
    "cnb_accuracy_score = metrics.accuracy_score(CNB.predict(X_test),Y_test)\n",
    "\n",
    "BNB = BernoulliNB()\n",
    "BNB.fit(X_train, Y_train)\n",
    "bnb_accuracy_score = metrics.accuracy_score(BNB.predict(X_test), Y_test)\n",
    "\n",
    "print('Stemmed Naive Bayes with 1-gram and 2-gram:')\n",
    "print(str('MNB Accuracy: {:04.2f}'.format(mnb_accuracy_score*100))+'%')\n",
    "print(str('CNB Accuracy: {:4.2f}'.format(cnb_accuracy_score*100))+'%')\n",
    "print(str('BNB Accuracy: {:4.2f}'.format(bnb_accuracy_score*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Naive Bayes with Tf-idf, 1-grams and 2-grams:\n",
      "MNB Accuracy: 80.26%\n",
      "CNB Accuracy: 80.25%\n",
      "BNB Accuracy: 79.54%\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range = (1,2),tokenizer = token.tokenize)\n",
    "text_count_2 = tfidf.fit_transform(tweet_df['Stemmed Text'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_count_2, tweet_df['Sentiment'],test_size=0.2,random_state=5)\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(x_train, y_train)\n",
    "predicted = MNB.predict(x_test)\n",
    "mnb_accuracy_score = metrics.accuracy_score(predicted, y_test)\n",
    "\n",
    "CNB = ComplementNB()\n",
    "CNB.fit(x_train, y_train)\n",
    "cnb_accuracy_score = metrics.accuracy_score(CNB.predict(x_test),y_test)\n",
    "\n",
    "BNB = BernoulliNB()\n",
    "BNB.fit(x_train, y_train)\n",
    "bnb_accuracy_score = metrics.accuracy_score(BNB.predict(x_test), y_test)\n",
    "\n",
    "print('Stemmed Naive Bayes with Tf-idf, 1-grams and 2-grams:')\n",
    "print(str('MNB Accuracy: {:04.2f}'.format(mnb_accuracy_score*100))+'%')\n",
    "print(str('CNB Accuracy: {:4.2f}'.format(cnb_accuracy_score*100))+'%')\n",
    "print(str('BNB Accuracy: {:4.2f}'.format(bnb_accuracy_score*100))+'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
